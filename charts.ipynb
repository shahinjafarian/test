{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASTLE Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from math import pi\n",
    "from collections import defaultdict\n",
    "\n",
    "import yaml\n",
    "import adjustText\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib_venn import venn3, venn3_circles\n",
    "import squarify\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "\n",
    "dataset_file = 'dataset.json'\n",
    "reports_directory = 'reports'\n",
    "report_suffix = '-report.json'\n",
    "report_version = '1.2' # the charts will only load the reports with this version!\n",
    "human_results = {\n",
    "    'name': 'Developers',\n",
    "    'type': 'human',\n",
    "    'tp': 11,\n",
    "    'fp': 10,\n",
    "}\n",
    "castle_scoring = {\n",
    "    'tp': 5,\n",
    "    'tn': 2,\n",
    "    'fp': -1,\n",
    "    'fn': 0,\n",
    "}\n",
    "castle_toplist_bonus = 5\n",
    "relevant_findings = [\n",
    "    '', 'none', # systems that do not include severity we treat them as equal\n",
    "    'warning', 'error', # systems using error type\n",
    "    'medium', 'high', 'critical', # systems using severity type\n",
    "    'portability' # portability-related findings often cause unexpected behavior\n",
    "]\n",
    "color_map = {\n",
    "    'sast': { 'color': '#648fff', 'label': 'Static Analyzer' },\n",
    "    'llm': { 'color': '#ffb347', 'label': 'LLM' },\n",
    "    'fv': { 'color': '#5bb9a9', 'label': 'Formal Verification' },\n",
    "}\n",
    "# https://www.color-hex.com/color-palette/1046567\n",
    "\n",
    "test_per_cwe = 10\n",
    "# 2024 (https://cwe.mitre.org/top25/archive/2024/2024_cwe_top25.html)\n",
    "top_25_cwes = [ 79, 787, 89, 352, 22, 125, 78, 416, 862, 434, 94, 20, 77, 287, 269, 502, 200, 863, 918, 119, 476, 798, 190, 400, 306 ]\n",
    "\n",
    "# Display names\n",
    "display_names = {\n",
    "    # Tools\n",
    "    'aikido': 'Aikido',\n",
    "    'clang-analyzer': 'Clang Analyzer',\n",
    "    'codeql': 'CodeQL',\n",
    "    'codethreat': 'CodeThreat',\n",
    "    'cppcheck': 'Cppcheck',\n",
    "    'gcc-fanalyzer': 'GCC Fanalyzer',\n",
    "    'gitlab-sast': 'GitLab SAST',\n",
    "    'jit': 'Jit',\n",
    "    'semgrep-code': 'Semgrep Code',\n",
    "    'snyk': 'Snyk',\n",
    "    'sonarqube': 'SonarQube',\n",
    "    'splint': 'Splint',\n",
    "    'coverity': 'Coverity',\n",
    "    \n",
    "    # BMCs\n",
    "    'esbmc': 'ESBMC',\n",
    "    'cbmc': 'CBMC',\n",
    "    \n",
    "    # LLMs\n",
    "    'falcon3-7b': 'Falcon 3 (7B)',\n",
    "    'gemma-2-9b': 'Gemma 2 (9B)',\n",
    "    'gpt-4o-mini': 'GPT-4o Mini',\n",
    "    'gpt-4o': 'GPT-4o',\n",
    "    'o1': 'GPT-o1',\n",
    "    'o3-mini': 'GPT-o3 Mini',\n",
    "    'qwen-2.5-coder-32b-instruct': 'QWEN 2.5CI (32B)',\n",
    "    'deepseek-reasoner': 'DeepSeek R1',\n",
    "    'llama-3.1-8b': 'LLAMA 3.1 (8B)',\n",
    "    'mistral-7b-ins': 'Mistral Ins. (7B)',\n",
    "}\n",
    "tool_color_map = {\n",
    "    'sast': '#648fff',\n",
    "    'gca': '#18795b',\n",
    "    'fv': '#5bb9a9',\n",
    "    'llm': '#ffcc00',\n",
    "    'combination': '#ffb347'\n",
    "}\n",
    "\n",
    "tool_type_map = {\n",
    "    # Tools\n",
    "    'aikido': 'sast',\n",
    "    'clang-analyzer': 'gca',\n",
    "    'codeql': 'sast',\n",
    "    'codethreat': 'sast',\n",
    "    'cppcheck': 'gca',\n",
    "    'gcc-fanalyzer': 'gca',\n",
    "    'gitlab-sast': 'sast',\n",
    "    'jit': 'sast',\n",
    "    'semgrep-code': 'sast',\n",
    "    'snyk': 'sast',\n",
    "    'sonarqube': 'sast',\n",
    "    'splint': 'sast',\n",
    "    'coverity': 'sast',\n",
    "    \n",
    "    # BMCs\n",
    "    'esbmc': 'fv',\n",
    "    'cbmc': 'fv',\n",
    "    \n",
    "    # LLms\n",
    "    'falcon3-7b': 'llm',\n",
    "    'gemma-2-9b': 'llm',\n",
    "    'gpt-4o-mini': 'llm',\n",
    "    'gpt-4o': 'llm',\n",
    "    'o1': 'llm',\n",
    "    'o3-mini': 'llm',\n",
    "    'qwen-2.5-coder-32b-instruct': 'llm',\n",
    "    'deepseek-reasoner': 'llm',\n",
    "    'llama-3.1-8b': 'llm',\n",
    "    'mistral-7b-ins': 'llm',\n",
    "}\n",
    "\n",
    "manual_version_map = {\n",
    "    # Tools\n",
    "    'aikido': '-',\n",
    "    'clang-analyzer': '18.1.3',\n",
    "    'codeql': '2.20.1',\n",
    "    'codethreat': '-',\n",
    "    'cppcheck': '2.13.0',\n",
    "    'gcc-fanalyzer': '13.3.0',\n",
    "    'gitlab-sast': '15.2.1',\n",
    "    'jit': '-',\n",
    "    'semgrep-code': '1.110.0',\n",
    "    'snyk': '1.1295.4',\n",
    "    'sonarqube': '25.3.0',\n",
    "    'splint': '3.1.2',\n",
    "    'coverity': '2024.12.1',\n",
    "    \n",
    "    # BMCs\n",
    "    'esbmc': '7.8.1',\n",
    "    'cbmc': '5.95.1',\n",
    "    \n",
    "    # LLms\n",
    "    'falcon3-7b': '-',\n",
    "    'gemma-2-9b': '-',\n",
    "    'gpt-4o-mini': '-',\n",
    "    'gpt-4o': '-',\n",
    "    'o1': '-',\n",
    "    'o3-mini': '-',\n",
    "    'qwen-2.5-coder-32b-instruct': '-',\n",
    "    'deepseek-reasoner': '-',\n",
    "    'llama-3.1-8b': '-',\n",
    "    'mistral-7b-ins': '-',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "# Load dataset\n",
    "with open(dataset_file) as f:\n",
    "    dataset = json.load(f)\n",
    "print(f\"Loaded dataset with {len(dataset['tests'])} tests.\")\n",
    "\n",
    "\n",
    "# Load all reports\n",
    "names = [\n",
    "    report.removesuffix(report_suffix)\n",
    "    for report\n",
    "    in os.listdir(reports_directory)\n",
    "    if report.endswith(report_suffix)\n",
    "]\n",
    "\n",
    "reports = {}\n",
    "for name in names:\n",
    "    with open( os.path.join(reports_directory, f'{name}{report_suffix}')) as f:\n",
    "        report = json.load(f)\n",
    "        if report['version'] != report_version:\n",
    "            print(f\"[!!!!] Skipping report {name} with version {report['version']} (expected {report_version})\")\n",
    "            continue\n",
    "        reports[name] = report\n",
    "print(f\"Loaded {len(reports)} reports.\")\n",
    "\n",
    "\n",
    "# Load CWE dictionary\n",
    "cwes = None\n",
    "with open('cwe-collection.yaml') as f:\n",
    "    cwes = yaml.load(f, Loader=yaml.FullLoader)\n",
    "print(f\"Loaded {len(cwes)} CWEs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate general stats\n",
    "total_vulnerable = len([ t for t in dataset['tests'] if t['vulnerable'] ])\n",
    "total_not_vulnerable = len(dataset['tests']) - total_vulnerable\n",
    "present_cwes = list(set([ t['cwe'] for t in dataset['tests'] if t['cwe'] != 0 ]))\n",
    "\n",
    "total_vulnerable, total_not_vulnerable, len(cwes), len(present_cwes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Helper Functions & Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helpers\n",
    "\n",
    "# Define the normalized CASTLE score\n",
    "# legacy V1\n",
    "#def castle_norm(tp, tn, fp, fn, w=5):\n",
    "#    return ( tp - (fp/w) ) / ( tp + fp + fn )\n",
    "\n",
    "def castle_combination(tp, tn, fp, fn, bonus):\n",
    "    return (\n",
    "        tp * castle_scoring['tp'] +\n",
    "        tn * castle_scoring['tn'] +\n",
    "        fp * castle_scoring['fp'] +\n",
    "        fn * castle_scoring['fn'] +\n",
    "        bonus\n",
    "    )\n",
    "\n",
    "def castle(cwe_toplist, toplist_bonus, results):\n",
    "    tps = 0\n",
    "    tns = 0\n",
    "    fps = 0\n",
    "    fns = 0\n",
    "    bonus = 0\n",
    "    for res in results:\n",
    "        tps += res['result']['tp']\n",
    "        tns += res['result']['tn']\n",
    "        fps += res['result']['fp']\n",
    "        fns += res['result']['fn']\n",
    "        assert len(res['expected']['cwe']) > 0, res\n",
    "        cwe = res['expected']['cwe'][0]\n",
    "        assert cwe > 0\n",
    "        if cwe in cwe_toplist and tps > 0:\n",
    "            bonus += toplist_bonus - cwe_toplist.index(cwe) // toplist_bonus\n",
    "    return castle_combination(tps, tns, fps, fns, bonus), tps, tns, fps, fns, bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cwes = list(set([test['cwe'] for test in dataset['tests']]))\n",
    "all_perfect_tests = []\n",
    "for cwe in all_cwes:\n",
    "    true_positive = {\n",
    "        'result': { 'tp': 1, 'tn': 0, 'fp': 0, 'fn': 0 },\n",
    "        'expected': { 'cwe': [cwe] }\n",
    "    }\n",
    "    true_negative = {\n",
    "        'result': { 'tp': 0, 'tn': 1, 'fp': 0, 'fn': 0 },\n",
    "        'expected': { 'cwe': [cwe] }\n",
    "    }\n",
    "    all_perfect_tests += [ true_positive ] * (total_vulnerable // len(all_cwes)) + [ true_negative ] * (total_not_vulnerable // len(all_cwes))\n",
    "score, tp, tn, fp, fn, bonus = castle(top_25_cwes, castle_toplist_bonus, all_perfect_tests)\n",
    "\n",
    "print(score, tp, tn, fp, fn, bonus)\n",
    "perfect_castle_score = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parse reports\n",
    "\n",
    "# Parse CWEs\n",
    "def cwe_collection_to_dict(cwe_collection: dict[str, any]) -> dict[int, list[int]]:\n",
    "    cwe_dict = {\n",
    "        0: []\n",
    "    }\n",
    "    for c in cwe_collection:\n",
    "        cwes = [ int(c) ]\n",
    "        cwes += [ int(list(p.keys())[0]) for p in cwe_collection[c]['children'] ]\n",
    "        cwes += [ int(list(p.keys())[0]) for p in cwe_collection[c]['parents'] ]\n",
    "        cwe_dict[int(c)] = cwes\n",
    "    return cwe_dict\n",
    "accepted_cwe_lists = cwe_collection_to_dict(dataset['cwes'])\n",
    "\n",
    "# Parser functions\n",
    "def filter_findings(parsed: list[dict]):\n",
    "    filtered = [ f for f in parsed if f['severity'].lower() in relevant_findings ]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def is_cwe_match(collection, correct: int, reported: int):\n",
    "    # Matches the correct CWE, any of its children or its direct parent\n",
    "    if reported in collection[correct]:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def is_line_match(corrects: list[int], reported: int):\n",
    "    if reported == 0:\n",
    "        return False\n",
    "    return reported in corrects\n",
    "\n",
    "def validate_findings(test: list[dict], findings: list[dict]):\n",
    "    # True negative\n",
    "    if len(findings) == 0 and not test['vulnerable']:\n",
    "        return {\n",
    "            \"id\": test['id'],\n",
    "            \"actual\": {\n",
    "                \"vulnerable\": False,\n",
    "                \"line\": 0,\n",
    "                \"cwe\": 0,\n",
    "                \"message\": '',\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"vulnerable\": test['vulnerable'],\n",
    "                \"line\": test['lines'],\n",
    "                \"cwe\": accepted_cwe_lists[test['cwe']],\n",
    "                \"message\": test['description'],\n",
    "            },\n",
    "            \"result\": {\n",
    "                'tp': 0,\n",
    "                'fp': 0,\n",
    "                'tn': 1,\n",
    "                'fn': 0,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    # Filter true findings\n",
    "    #matches = [ f for f in findings if f['line'] in test['lines'] or f['cwe'] in accepted_cwe_lists[test['cwe']] ]\n",
    "    matches = []\n",
    "    for f in findings:\n",
    "        # Skip overrides to false and include overrides to true\n",
    "        if 'override' in f:\n",
    "            if f['override'] == True:\n",
    "                matches.append(f)\n",
    "            continue\n",
    "        \n",
    "        # Include if line or CWE matches\n",
    "        if is_line_match(test['lines'], f['line']) or is_cwe_match(accepted_cwe_lists, test['cwe'], f['cwe']):\n",
    "            matches.append(f)\n",
    "    tp_count = len(matches)\n",
    "    fp_count = len(findings) - tp_count\n",
    "        \n",
    "    # False negative\n",
    "    if tp_count == 0 and test['vulnerable']:\n",
    "        return {\n",
    "            \"id\": test['id'],\n",
    "            \"actual\": {\n",
    "                \"vulnerable\": False,\n",
    "                \"line\": 0,\n",
    "                \"cwe\": 0,\n",
    "                \"message\": '',\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"vulnerable\": test['vulnerable'],\n",
    "                \"line\": test['lines'],\n",
    "                \"cwe\": accepted_cwe_lists[test['cwe']],\n",
    "                \"message\": test['description'],\n",
    "            },\n",
    "            \"result\": {\n",
    "                'tp': 0,\n",
    "                'fp': fp_count,\n",
    "                'tn': 0,\n",
    "                'fn': 1,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    # True positive\n",
    "    if tp_count > 0:\n",
    "        m = matches[0]\n",
    "        false_positive_count = fp_count # if two findings are reported on the same line, we count it as one, so we are not subtracting points. Sone systems may report on an issue in the given line multiple times, because they separate out the underlying cause and its effects. We are not deducting points for that.\n",
    "        return {\n",
    "            \"id\": test['id'],\n",
    "            \"actual\": {\n",
    "                \"vulnerable\": True,\n",
    "                \"line\": m['line'],\n",
    "                \"cwe\": m['cwe'],\n",
    "                \"message\": m['message'],\n",
    "            },\n",
    "            \"expected\": {\n",
    "                \"vulnerable\": test['vulnerable'],\n",
    "                \"line\": test['lines'],\n",
    "                \"cwe\": accepted_cwe_lists[test['cwe']],\n",
    "                \"message\": test['description'],\n",
    "            },\n",
    "            \"result\": {\n",
    "                'tp': 1,\n",
    "                'fp': false_positive_count,\n",
    "                'tn': 0,\n",
    "                'fn': 0,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    # False positive\n",
    "    return {\n",
    "        \"id\": test['id'],\n",
    "        \"actual\": {\n",
    "            \"vulnerable\": False,\n",
    "            \"line\": 0,\n",
    "            \"cwe\": 0,\n",
    "            \"message\": '',\n",
    "        },\n",
    "        \"expected\": {\n",
    "            \"vulnerable\": test['vulnerable'],\n",
    "            \"line\": test['lines'],\n",
    "            \"cwe\": accepted_cwe_lists[test['cwe']],\n",
    "            \"message\": test['description'],\n",
    "        },\n",
    "        \"result\": {\n",
    "            'tp': 0,\n",
    "            'fp': fp_count,\n",
    "            'tn': 0,\n",
    "            'fn': 0,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "print(f'Processing {len(reports)} reports...')\n",
    "results = {}\n",
    "for i,name in enumerate(reports):\n",
    "    print(f'   [{i+1}/{len(reports)}] {name}')\n",
    "    results[name] = []\n",
    "    for i, rep in enumerate(reports[name]['tests']):\n",
    "        # Remove low level / irrelevant findings with exceptions\n",
    "        findings = filter_findings(rep['findings'])\n",
    "        # Determine result\n",
    "        result = validate_findings(dataset['tests'][i], findings)\n",
    "        results[name].append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CWE toplist standings\n",
    "cwe_toplist = {}\n",
    "for cwe in accepted_cwe_lists:\n",
    "    cwe_toplist[cwe] = -1\n",
    "    for c in accepted_cwe_lists[cwe]:\n",
    "        if c in top_25_cwes:\n",
    "            index = top_25_cwes.index(c)\n",
    "            if cwe_toplist[cwe] == -1 or index < cwe_toplist[cwe]:\n",
    "                cwe_toplist[cwe] = index\n",
    "    \n",
    "cwe_toplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASTLE\n",
    "scores = {}\n",
    "for name in results:\n",
    "    castle_score, tps, tns, fps, fns, bonus = castle(top_25_cwes, castle_toplist_bonus, results[name])\n",
    "    scores[name] = {\n",
    "        'tp': tps,\n",
    "        'tn': tns,\n",
    "        'fp': fps,\n",
    "        'fn': fns,\n",
    "        'castle': castle_score,\n",
    "        'precision': tps / (tps + fps) if tps + fps > 0 else 0, # avoid division by zero\n",
    "        'recall': tps / (tps + fns) if tps + fns > 0 else 0, # recall is the same thing as 'coverage'\n",
    "        'accuracy': (tps + tns) / (tps + tns + fps + fns), # how many did it get right out of all predictions\n",
    "        'type': tool_type_map[name],\n",
    "    }\n",
    "\n",
    "scores['aikido']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of true positives for each test. CASTLE-1, CASTLE-2, etc.\n",
    "tp_counts = defaultdict(int)\n",
    "for name in results:\n",
    "    for finding in results[name]:\n",
    "        if finding['result']['tp'] == 1:\n",
    "            tp_counts[ finding['id'] ] += 1\n",
    "\n",
    "sorted_tp_counts = sorted(tp_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "sorted_tp_counts[:10], sorted_tp_counts[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cwes:\n",
    "    cwes[i]['test_count'] = 0\n",
    "\n",
    "for test in dataset['tests']:\n",
    "    cwes[ str(test['cwe']) ]['test_count'] += 1\n",
    "\n",
    "ordered_cwes = { k: cwes[k] for k in sorted(cwes, key=lambda k: int(k), reverse=False) }\n",
    "\n",
    "for i in ordered_cwes:\n",
    "    top_index = cwe_toplist[int(i)]\n",
    "    name = ordered_cwes[i]['name'].split(\"('\")[0]\n",
    "    print(f'CWE-{i} & {top_index+1 if top_index != -1 else '-'} & {name} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination score\n",
    "def combination_score(results, a, b):\n",
    "    resa = results[a]\n",
    "    resb = results[b]\n",
    "    assert len(resa) == len(resb), f\"Test count mismatch: {len(resa)} != {len(resb)}, {a}:{b}\"\n",
    "    \n",
    "    combined = []\n",
    "    for i in range(len(resa)):\n",
    "        assert resa[i]['id'] == resb[i]['id'], f\"Test ID mismatch at index {i}: {resa[i]['id']} != {resb[i]['id']}, {a}:{b}\"\n",
    "        combined.append({\n",
    "            'name': resa[i]['id'],\n",
    "            'result': {\n",
    "                'tp': resa[i]['result']['tp'] == 1 or resb[i]['result']['tp'] == 1,\n",
    "                'tn': resa[i]['result']['tn'] == 1 and resb[i]['result']['tn'] == 1,\n",
    "                'fp': resa[i]['result']['fp'] + resb[i]['result']['fp'],\n",
    "                'fn': resa[i]['result']['fn'] == 1 or resb[i]['result']['fn'] == 1,\n",
    "            },\n",
    "            'expected': {\n",
    "                'cwe': resa[i]['expected']['cwe'],\n",
    "            },\n",
    "        })\n",
    "    \n",
    "    return castle(top_25_cwes, castle_toplist_bonus, combined)\n",
    "\n",
    "combination_score(results, 'codeql', 'aikido')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all matrix combinations\n",
    "combinations = []\n",
    "for a in results:\n",
    "    for b in results:\n",
    "        if a == b:\n",
    "            continue\n",
    "        combinations.append({\n",
    "            'a': a,\n",
    "            'b': b,\n",
    "            'score': combination_score(results, a, b)[0]\n",
    "        })\n",
    "        \n",
    "len(combinations), combinations[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tests per CWE\n",
    "x = [ cwe for cwe in ordered_cwes ]\n",
    "y = [ ordered_cwes[cwe]['test_count'] for cwe in ordered_cwes ]\n",
    "\n",
    "plt.bar(x, y)\n",
    "plt.title('Number of Tests per CWE')\n",
    "plt.xlabel('CWE')\n",
    "plt.ylabel('Number of Tests')\n",
    "\n",
    "# set plot size\n",
    "fig = plt.gcf()\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart of color-coded castle scores\n",
    "scores_sorted = sorted(scores.items(), key=lambda item: item[1]['castle'], reverse=False)\n",
    "\n",
    "x = [ display_names[ s[0] ] for s in scores_sorted ]\n",
    "y = [ s[1]['castle'] for s in scores_sorted ]\n",
    "colors = [tool_color_map[tool_type_map[s[0]]] for s in scores_sorted]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar = ax.bar(x, y, color=colors)\n",
    "plt.ylabel('CASTLE Score')\n",
    "\n",
    "ax.set_xticks(range(len(x)), labels=x, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [ Patch(facecolor=color_map[color]['color'], label=color_map[color]['label']) for color in color_map ]\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(0.95, 1.1), ncol=3)\n",
    "\n",
    "# set plot size\n",
    "fig = plt.gcf()\n",
    "\n",
    "plt.savefig('assets/castle-scores.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart of castle scores divided between SCAs and LLMs\n",
    "scores_sca = { k: v for k, v in scores.items() if v['type'] in [ 'sast', 'bmc' ] }\n",
    "scores_llm = { k: v for k, v in scores.items() if v['type'] == 'llm' }\n",
    "print(f\"SCAs: {len(scores_sca)}, LLMs: {len(scores_llm)}\")\n",
    "\n",
    "scores_sca_sorted = sorted(scores_sca.items(), key=lambda item: item[1]['castle'], reverse=False)\n",
    "scores_llm_sorted = sorted(scores_llm.items(), key=lambda item: item[1]['castle'], reverse=False)\n",
    "\n",
    "x_sca = [ display_names[ s[0] ] for s in scores_sca_sorted ]\n",
    "y_sca = [ s[1]['castle'] for s in scores_sca_sorted ]\n",
    "\n",
    "x_llm = [ display_names[ s[0] ] for s in scores_llm_sorted ]\n",
    "y_llm = [ s[1]['castle'] for s in scores_llm_sorted ]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw={'width_ratios': [len(scores_sca), len(scores_llm)]})\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "\n",
    "bar = ax.bar(x_sca, y_sca)\n",
    "plt.ylim(-800, 1000)\n",
    "plt.ylabel('CASTLE Score')\n",
    "\n",
    "ax.set_xticks(range(len(x_sca)), labels=x_sca, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "\n",
    "## 2: LLMs\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "bar = ax.bar(x_llm, y_llm)\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.yaxis.tick_right()\n",
    "\n",
    "ax.set_xticks(range(len(x_llm)), labels=x_llm, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "\n",
    "plt.savefig('assets/castle-scores-separate.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for number of True positives vs False negatives\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "x = [ scores[e]['fp'] if scores[e]['fp'] != 0 else 0.1 for e in scores if scores[e]['type'] != 'llm' ] # avoid log(0) error\n",
    "y = [ scores[e]['tp'] for e in scores if scores[e]['type'] != 'llm' ]\n",
    "colors = [ tool_color_map[ tool_type_map[e] ] for e in scores if scores[e]['type'] != 'llm' ]\n",
    "names = [ e for e in scores if scores[e]['type'] != 'llm' ]\n",
    "\n",
    "# Base plot\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "ax.scatter(x, y,\n",
    "    c=colors,\n",
    "    marker='o', # https://matplotlib.org/stable/api/markers_api.html#module-matplotlib.markers\n",
    ")\n",
    "\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('False Positives (lower is better)')\n",
    "ax.set_ylabel('True Positives (higher is better)')\n",
    "ax.set_xscale(\"log\")\n",
    "labels = []\n",
    "fig.figure.set_size_inches(8, 4)\n",
    "for i,n in enumerate(names):\n",
    "    display_name = display_names[n]\n",
    "    labels += [ ax.text(x[i], y[i], display_name) ]\n",
    "\n",
    "adjustText.adjust_text(labels, x=x, y=y, ax=ax, expand_points=(1.2, 1.2), expand_text=(1.2, 1.2), force_text=(0.5, 0.5), force_points=(0.5, 0.5))\n",
    "\n",
    "x_min, x_max = ax.get_xlim()\n",
    "y_min, y_max = ax.get_ylim()\n",
    "\n",
    "# Generate x values in logarithmic space\n",
    "x_values = np.logspace(np.log10(x_min), np.log10(x_max), 100)\n",
    "#x_values = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "# Corresponding y values (1:1 ratio)\n",
    "#y_values = np.clip(x_values, y_min, y_max)  # Ensures y-values do not exceed plot limits\n",
    "y_values = np.clip(x_values, y_min, y_max)  # Ensures y-values do not exceed plot limits\n",
    "\n",
    "# Plot the 1:1 reference line\n",
    "ax.plot(x_values, y_values, linestyle=\"--\", color=\"gray\", label=\"1:1 Ratio Line\")\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "# Legend\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=tool_color_map['sast'], label='Static Application Security Tester'),\n",
    "    Patch(facecolor=tool_color_map['gca'], label='Generic Code Analyzer'),\n",
    "    Patch(facecolor=tool_color_map['fv'], label='Formal Verification'),\n",
    "    #Patch(facecolor=tool_color_map['combination'], label='Tool Combination'),\n",
    "    Patch(facecolor='none', edgecolor='gray', linestyle='--', label=\"1:1 Ratio Line\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(0.85, 1.25), ncol=2)\n",
    "\n",
    "plt.savefig('assets/tp-fp-scatter.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASTLE Scores of tool combinations (/10)\n",
    "x = results.keys()\n",
    "y = results.keys()\n",
    "\n",
    "data = np.array([ [ int(combination_score(results, a, b)[0]/10) for a in x ] for b in y ])\n",
    "mask = np.tri(data.shape[0], k=0).T\n",
    "data = np.ma.array(data, mask=mask)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# Filter out combinations of the same tools and ensure each combination is only shown once\n",
    "im = ax.imshow(data)\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(range(len(y)), labels=y,\n",
    "              rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.set_yticks(range(len(x)), labels=x)\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        if i > j: # Only show labels on lower triangle\n",
    "            text = ax.text(j, i, data[i, j], ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"CASTLE Score of Tool Combinations\")\n",
    "fig.tight_layout()\n",
    "fig.figure.set_size_inches(10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a midpoint normalization generator for matplotlib heatmaps\n",
    "class MidpointNormalize(matplotlib.colors.Normalize):\n",
    "    def __init__(self, vmin, vmax, midpoint=0, gamma=1.0, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        self.gamma = gamma\n",
    "        super().__init__(vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        result, is_scalar = self.process_value(value)\n",
    "        vmin, vmax, midpoint, gamma = self.vmin, self.vmax, self.midpoint, self.gamma\n",
    "\n",
    "        # Initialize array for normalized values\n",
    "        rescaled = np.empty_like(result)\n",
    "        # Normalize positive values\n",
    "        pos_mask = result >= midpoint\n",
    "        rescaled[pos_mask] = 0.5 + 0.5 * ((result[pos_mask] - midpoint) / (vmax - midpoint))**gamma\n",
    "        # Normalize negative values\n",
    "        neg_mask = result < midpoint\n",
    "        rescaled[neg_mask] = 0.5 - 0.5 * ((midpoint - result[neg_mask]) / (midpoint - vmin))**gamma\n",
    "\n",
    "        return np.ma.array(rescaled, mask=np.ma.getmask(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score delta from higer tool score\n",
    "\n",
    "x = list([r for r in results if scores[r]['type'] != 'llm'])\n",
    "y = list([r for r in results if scores[r]['type'] != 'llm'])\n",
    "\n",
    "data = np.array([\n",
    "    [\n",
    "        combination_score(results, a, b)[0] - max( scores[a]['castle'], scores[b]['castle'])\n",
    "        # int(round( ( (combination_score(results, a, b)[0] - max(scores[a]['castle'], scores[b]['castle'])) / max(scores[a]['castle'], scores[b]['castle']) ) * 100, 0))\n",
    "        for a in x\n",
    "    ] for b in y\n",
    "])\n",
    "\n",
    "# Mask the upper right triangle to not show duplicates\n",
    "mask = np.tri(data.shape[0], k=0).T\n",
    "data = np.ma.array(data, mask=mask)\n",
    "data.set_fill_value(1)  # Set fill value for masked elements\n",
    "masked_data = np.ma.filled(data, fill_value=0)  # Replace masked elements with NaN\n",
    "cmap = plt.cm.RdYlGn\n",
    "cmap.set_bad(color='white')  # Set the color for masked elements to white\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "norm = MidpointNormalize(vmin=np.min(data), vmax=np.max(data), midpoint=0, gamma=0.5)\n",
    "im = ax.imshow(data, cmap=\"RdYlGn\", norm=norm) # Diverging bymonotonic colormap\n",
    "\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(range(len(y)), labels=[ display_names[name] for name in results if scores[name]['type'] != 'llm' ], rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.set_yticks(range(len(x)), labels=[ display_names[name] for name in results if scores[name]['type'] != 'llm' ])\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        if i >= j:\n",
    "            text = str(data[i, j])\n",
    "            font_size = 10\n",
    "            text_color = \"w\"\n",
    "            \n",
    "            if data[i, j] > 0:\n",
    "                text = f'+{text}'\n",
    "            if len(text) >= 4:\n",
    "                font_size = 8\n",
    "            if abs(data[i, j]) < 25:\n",
    "                text_color = \"black\"\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color='black', fontsize=8, rotation=45)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.figure.set_size_inches(6, 6)\n",
    "ax.set_rasterization_zorder(1) # This solves an issue where the masked background was black when exported to eps\n",
    "\n",
    "plt.savefig('assets/combination-delta.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combination toplist\n",
    "combination_scores = [ [ (a, b, combination_score(results, a, b)[0]) for a in x ] for b in y ]\n",
    "combination_scores = [ item for sublist in combination_scores for item in sublist ] # flatten\n",
    "\n",
    "combination_scores = sorted(combination_scores, key=lambda x: x[2], reverse=True)\n",
    "# filter duplicate combinations\n",
    "combination_scores = [ c for c in combination_scores if c[0] < c[1] ]\n",
    "combination_scores[30:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toplist for Latex\n",
    "toplist = [ { 'name': e, **scores[e] } for e in scores ]\n",
    "tool_toplist = sorted([t for t in toplist if tool_type_map[t['name']] != 'llm' ], key=lambda x: x['castle'], reverse=True)\n",
    "llm_toplist = sorted([t for t in toplist if tool_type_map[t['name']] == 'llm' ], key=lambda x: x['castle'], reverse=True)\n",
    "\n",
    "norm = MidpointNormalize(vmin=-1000, vmax=1250, midpoint=0, gamma=0.5)\n",
    "colormap = plt.get_cmap(\"RdYlGn\")\n",
    "\n",
    "def cellcolor(value):\n",
    "    color = colormap(value)\n",
    "    return f'\\\\cellcolor[rgb]{{{color[0]:.4f}, {color[1]:.4f}, {color[2]:.4f}}}'\n",
    "    \n",
    "def print_toplist(toplist):\n",
    "    for i in toplist:\n",
    "        precision = round(i['precision']*100, 1)\n",
    "        recall = round(i['recall']*100, 1)\n",
    "        accuracy = round(i['accuracy']*100, 1)\n",
    "        \n",
    "        castle_score = round(i['castle'])\n",
    "        color = cellcolor(castle_score/977)\n",
    "        \n",
    "        version = manual_version_map[i['name']]\n",
    "        if version == None:\n",
    "            version = '-'\n",
    "        \n",
    "        print(f'{ display_names[i[\"name\"]] } & {version} & {i[\"tp\"]} & {i[\"tn\"]} & {i[\"fp\"]} & {i[\"fn\"]} & {precision:.0f}\\\\% & {recall:.0f}\\\\% & {accuracy:.0f}\\\\% & {castle_score} {color} \\\\\\\\')\n",
    "\n",
    "print_toplist(tool_toplist)\n",
    "print('\\\\hline \\\\hline')\n",
    "print_toplist(llm_toplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cubic combinations\n",
    "\n",
    "def cubic_combination_score(results, a, b, c):\n",
    "    resa = results[a]\n",
    "    resb = results[b]\n",
    "    resc = results[c]\n",
    "    assert len(resa) == len(resb) == len(resc), f\"Test count mismatch: {len(resa)} != {len(resb)} != {len(resc)}\"\n",
    "    \n",
    "    combined = []\n",
    "    for i in range(len(resa)):\n",
    "        assert resa[i]['id'] == resb[i]['id'] == resc[i]['id'], f\"Test ID mismatch at index {i}: {resa[i]['id']} != {resb[i]['id']} != {resa[i]['id']}\"\n",
    "        combined.append({\n",
    "            'name': resa[i]['id'],\n",
    "            'result': {\n",
    "                'tp': resa[i]['result']['tp'] == 1 or resb[i]['result']['tp'] == 1 or resc[i]['result']['tp'] == 1,\n",
    "                'tn': resa[i]['result']['tn'] == 1 and resb[i]['result']['tn'] == 1 and resc[i]['result']['tn'] == 1,\n",
    "                'fp': resa[i]['result']['fp'] + resb[i]['result']['fp'] + resc[i]['result']['fp'],\n",
    "                'fn': resa[i]['result']['fn'] == 1 or resb[i]['result']['fn'] == 1 or resc[i]['result']['fn'] == 1,\n",
    "            },\n",
    "            'expected': {\n",
    "                'cwe': resa[i]['expected']['cwe'],\n",
    "            },\n",
    "        })\n",
    "    \n",
    "    castle_score, tps, tns, fps, fns, bonus = castle(top_25_cwes, castle_toplist_bonus, combined)\n",
    "    return castle_score\n",
    "\n",
    "combination_score(results, 'codeql', 'aikido')[0]\n",
    "\n",
    "cubic_combinations = []\n",
    "for a in results:\n",
    "    for b in results:\n",
    "        for c in results:\n",
    "            if a == b or a == c or b == c:\n",
    "                continue\n",
    "            order = list(results.keys())\n",
    "            if order.index(a) > order.index(b) or order.index(b) > order.index(c):\n",
    "                continue\n",
    "            cubic_combinations.append({\n",
    "                'a': a,\n",
    "                'b': b,\n",
    "                'c': c,\n",
    "                'best': max(scores[a]['castle'], scores[b]['castle'], scores[c]['castle']),\n",
    "                'score': cubic_combination_score(results, a, b, c)\n",
    "            })\n",
    "            \n",
    "# Filter out worse results\n",
    "cubic_combinations = [ c for c in cubic_combinations if c['score'] > c['best'] ]\n",
    "# Sort by biggest delta\n",
    "cubic_combinations = sorted(cubic_combinations, key=lambda x: abs(x['score'] - x['best']), reverse=True)\n",
    "\n",
    "print(len(cubic_combinations))\n",
    "for i in cubic_combinations[:25]:\n",
    "    print(f'{i[\"a\"]} & {i[\"b\"]} & {i[\"c\"]} ||| best={i[\"best\"]} togeher={i[\"score\"]} (Δ {i[\"score\"] - i[\"best\"]}) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix for vulnerability coverage\n",
    "cwe_list = list(ordered_cwes.keys())\n",
    "tool_list = list(results.keys())\n",
    "\n",
    "coverage_matrix = np.zeros((len(cwe_list), len(tool_list)))\n",
    "\n",
    "# Fill the matrix with coverage data\n",
    "for i, cwe in enumerate(cwe_list):\n",
    "    for j, tool in enumerate(tool_list):\n",
    "        for test in results[tool]:\n",
    "            if test['expected']['cwe'][0] == int(cwe) and test['expected']['vulnerable'] and test['result']['tp'] == 1:\n",
    "                coverage_matrix[i, j] += 1\n",
    "\n",
    "# Plot the heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "im = ax.imshow(coverage_matrix, cmap=\"YlGnBu\")\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(tool_list)), labels=[ display_names[name] for name in results ], rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.set_yticks(np.arange(len(cwe_list)), labels=[f'CWE-{cwe}' for cwe in cwe_list])\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(cwe_list)):\n",
    "    for j in range(len(tool_list)):\n",
    "        count = int(coverage_matrix[i, j])\n",
    "        text = str(count)\n",
    "        if count > 4:\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\")\n",
    "        else:\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('assets/tp-heatmap.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix for vulnerability coverage, separated by tool type\n",
    "cwe_list = list(ordered_cwes.keys())\n",
    "tool_list = [ r for r in results if scores[r]['type'] != 'llm'  ]\n",
    "\n",
    "coverage_matrix = np.zeros((len(cwe_list), len(tool_list)))\n",
    "\n",
    "# Fill the matrix with coverage data\n",
    "for i, cwe in enumerate(cwe_list):\n",
    "    for j, tool in enumerate(tool_list):\n",
    "        for test in results[tool]:\n",
    "            if test['expected']['cwe'][0] == int(cwe) and test['expected']['vulnerable'] and test['result']['tp'] == 1:\n",
    "                coverage_matrix[i, j] += 1\n",
    "\n",
    "# Plot the heatmap\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax = axs[0]\n",
    "im = ax.imshow(coverage_matrix, cmap=\"YlGnBu\")\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(tool_list)), labels=[ display_names[name] for name in tool_list ], rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.set_yticks(np.arange(len(cwe_list)), labels=[f'CWE-{cwe}' for cwe in cwe_list])\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(cwe_list)):\n",
    "    for j in range(len(tool_list)):\n",
    "        count = int(coverage_matrix[i, j])\n",
    "        text = str(count)\n",
    "        if count > 4:\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\")\n",
    "        else:\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "ax.title.set_text('Static Analysis Tools')\n",
    "\n",
    "## 2 LLM\n",
    "\n",
    "cwe_list = list(ordered_cwes.keys())\n",
    "tool_list = [ r for r in results if scores[r]['type'] == 'llm'  ]\n",
    "\n",
    "coverage_matrix = np.zeros((len(cwe_list), len(tool_list)))\n",
    "\n",
    "# Fill the matrix with coverage data\n",
    "for i, cwe in enumerate(cwe_list):\n",
    "    for j, tool in enumerate(tool_list):\n",
    "        for test in results[tool]:\n",
    "            if test['expected']['cwe'][0] == int(cwe) and test['expected']['vulnerable'] and test['result']['tp'] == 1:\n",
    "                coverage_matrix[i, j] += 1\n",
    "                \n",
    "\n",
    "# Plot the heatmap\n",
    "ax = axs[1]\n",
    "im = ax.imshow(coverage_matrix, cmap=\"YlGnBu\")\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(tool_list)), labels=[ display_names[name] for name in tool_list ], rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(cwe_list)):\n",
    "    for j in range(len(tool_list)):\n",
    "        count = int(coverage_matrix[i, j])\n",
    "        text = str(count)\n",
    "        if count > 4:\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\")\n",
    "        else:\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"black\")\n",
    "ax.title.set_text('Large Language Models')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0, wspace=-0.56)\n",
    "\n",
    "plt.savefig('assets/tp-heatmap-separate.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix for false positive rates on non-vulnerable tests\n",
    "cwe_list = list(ordered_cwes.keys())\n",
    "tool_list = list(results.keys())\n",
    "\n",
    "fp_rate_matrix = np.zeros((len(cwe_list), len(tool_list)))\n",
    "\n",
    "# Fill the matrix with false positive rate data\n",
    "for i, cwe in enumerate(cwe_list):\n",
    "    for j, tool in enumerate(tool_list):\n",
    "        total_non_vulnerable_tests = sum(1 for test in dataset['tests'] if test['cwe'] == int(cwe) and not test['vulnerable'])\n",
    "        if total_non_vulnerable_tests > 0:\n",
    "            false_positives = sum(1 for test in results[tool] if test['expected']['cwe'][0] == int(cwe) and not test['expected']['vulnerable'] and test['result']['fp'] > 0)\n",
    "            fp_rate_matrix[i, j] = false_positives / total_non_vulnerable_tests\n",
    "\n",
    "# Plot the heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "im = ax.imshow(fp_rate_matrix, cmap=\"Reds\")\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(tool_list)), labels=[ display_names[name] for name in results ], rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "ax.set_yticks(np.arange(len(cwe_list)), labels=[f'CWE-{cwe}' for cwe in cwe_list])\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(cwe_list)):\n",
    "    for j in range(len(tool_list)):\n",
    "        rate = int(fp_rate_matrix[i, j] * 4)\n",
    "        text = str(rate)\n",
    "        if rate > 0.4:\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"white\")\n",
    "        else:\n",
    "            ax.text(j, i, text, ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('assets/fp-heatmap.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar charts for precision, recall and accuracy for all tools (DISABLED TO SAVE SPACE)\n",
    "def create_radar_chart(data, categories, title):\n",
    "    N = len(categories)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variables)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Draw one axe per variable + add labels\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\", \"0.4\", \"0.6\", \"0.8\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Ind1\n",
    "    values = list(data.values())\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)\n",
    "    \n",
    "    # Add a title\n",
    "    plt.title(title, size=20, color='b', y=1.1)\n",
    "\n",
    "# Define the categories\n",
    "categories = ['precision', 'recall', 'accuracy']\n",
    "\n",
    "# Create radar charts for each tool\n",
    "for tool in scores:\n",
    "    data = {\n",
    "        'precision': scores[tool]['precision'],\n",
    "        'recall': scores[tool]['recall'],\n",
    "        'accuracy': scores[tool]['accuracy'],\n",
    "    }\n",
    "    create_radar_chart(data, categories, tool)\n",
    "    #plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar charts for categorically combined tools\n",
    "# Function to create radar chart with multiple datasets\n",
    "def create_stacked_radar_chart(ax, datasets, categories, title):\n",
    "    N = len(categories)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variables)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Draw one axe per variable + add labels\n",
    "    plt.xticks(angles[:-1], categories)\n",
    "    \n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.2, 0.4, 0.6, 0.8], [\"0.2\", \"0.4\", \"0.6\", \"0.8\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Plot each dataset\n",
    "    for data in datasets:\n",
    "        values = list(data['values'].values())\n",
    "        values += values[:1]\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=data['label'])\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "    \n",
    "    # Add a title\n",
    "    plt.title(title, size=14, color='black', y=1.1)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.45), ncol=3)\n",
    "\n",
    "# Define the categories\n",
    "categories = ['precision', 'recall', 'accuracy']\n",
    "\n",
    "## ------- CHART 1\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5), subplot_kw=dict(polar=True))\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "# Create radar charts for selected tools\n",
    "# selected_tools = ['aikido', 'cppcheck', 'codeql']\n",
    "selected_tools = [ t for t in scores if scores[t]['type'] == 'llm' ]\n",
    "datasets = []\n",
    "for tool in selected_tools:\n",
    "    data = {\n",
    "        'label': display_names[tool],\n",
    "        'values': {\n",
    "            'precision': scores[tool]['precision'],\n",
    "            'recall': scores[tool]['recall'],\n",
    "            'accuracy': scores[tool]['accuracy'],\n",
    "        }\n",
    "    }\n",
    "    datasets.append(data)\n",
    "\n",
    "ax = plt.subplot(1, 2, 1, polar=True)\n",
    "create_stacked_radar_chart(ax, datasets, categories, \"Large Language Models\")\n",
    "\n",
    "## ------- CHART 2\n",
    "\n",
    "# Create radar charts for selected tools\n",
    "# selected_tools = ['aikido', 'cppcheck', 'codeql']\n",
    "selected_tools = [ t for t in scores if scores[t]['type'] != 'llm' ]\n",
    "datasets = []\n",
    "for tool in selected_tools:\n",
    "    data = {\n",
    "        'label': display_names[tool],\n",
    "        'values': {\n",
    "            'precision': scores[tool]['precision'],\n",
    "            'recall': scores[tool]['recall'],\n",
    "            'accuracy': scores[tool]['accuracy'],\n",
    "        }\n",
    "    }\n",
    "    datasets.append(data)\n",
    "ax = plt.subplot(1, 2, 2, polar=True)\n",
    "create_stacked_radar_chart(ax, datasets, categories, \"Static Analyzers\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('assets/radars.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Venn diagram for detected vulnerabilities for tool combinations\n",
    "\n",
    "# Function to get the set of detected vulnerabilities for a tool\n",
    "def get_detected_vulnerabilities(tool_name):\n",
    "    detected_vulnerabilities = set()\n",
    "    for test in results[tool_name]:\n",
    "        if test['result']['tp'] == 1:\n",
    "            detected_vulnerabilities.add(test['id'])\n",
    "    return detected_vulnerabilities\n",
    "\n",
    "# Select three tools for the Venn diagram\n",
    "tool_a = 'aikido'\n",
    "tool_b = 'clang-analyzer'\n",
    "tool_c = 'esbmc'\n",
    "\n",
    "# Get the sets of detected vulnerabilities\n",
    "set_a = get_detected_vulnerabilities(tool_a)\n",
    "set_b = get_detected_vulnerabilities(tool_b)\n",
    "set_c = get_detected_vulnerabilities(tool_c)\n",
    "\n",
    "# Create the Venn diagram\n",
    "plt.figure(figsize=(10, 7))\n",
    "venn = venn3([set_a, set_b, set_c], (tool_a, tool_b, tool_c))\n",
    "venn3_circles([set_a, set_b, set_c], linestyle='dashed')\n",
    "\n",
    "# Add title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square treemap of score components\n",
    "\n",
    "# Define the score components for the treemap\n",
    "score_components = {\n",
    "    'True Positives': tps,\n",
    "    'True Negatives': tns,\n",
    "    'False Positives': fps,\n",
    "    'False Negatives': fns,\n",
    "    'Toplist Bonus': bonus\n",
    "}\n",
    "\n",
    "# Define the sizes and labels for the treemap\n",
    "sizes = list(score_components.values())\n",
    "labels = [f'{key}\\n{value}' for key, value in score_components.items()]\n",
    "\n",
    "# Create the treemap\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "squarify.plot(sizes=sizes, label=labels, alpha=0.7, ax=ax)\n",
    "plt.title('Treemap of Score Components')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-and-whisker plots for score components\n",
    "\n",
    "# Extract score components for each tool\n",
    "tp_scores = [scores[tool]['tp'] for tool in scores]\n",
    "tn_scores = [scores[tool]['tn'] for tool in scores]\n",
    "fp_scores = [scores[tool]['fp'] for tool in scores]\n",
    "fn_scores = [scores[tool]['fn'] for tool in scores]\n",
    "bonus_scores = [castle(top_25_cwes, castle_toplist_bonus, results[tool])[5] for tool in scores]\n",
    "\n",
    "# Create a dictionary to hold the data for box plots\n",
    "score_data = {\n",
    "    'True Positives': tp_scores,\n",
    "    'True Negatives': tn_scores,\n",
    "    'False Positives': fp_scores,\n",
    "    'False Negatives': fn_scores,\n",
    "    'Toplist Bonus': bonus_scores\n",
    "}\n",
    "\n",
    "# Plot box-and-whisker charts\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(20, 5))\n",
    "fig.suptitle('Box-and-Whisker Plots for Score Components')\n",
    "\n",
    "for ax, (label, data) in zip(axes, score_data.items()):\n",
    "    ax.boxplot(data)\n",
    "    ax.set_title(label)\n",
    "    ax.set_ylabel('Scores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single, matrix and cubic combinations\n",
    "all_combinations = []\n",
    "all_combinations += [ ( [name], scores[name]['castle'], scores[name]['castle'] ) for name in scores ] # 1\n",
    "all_combinations += [ ( [cs[0], cs[1]], cs[2], max(scores[cs[0]]['castle'], scores[cs[1]]['castle']) ) for cs in combination_scores ] # 2\n",
    "#all_combinations += [ ( [cc['a'], cc['b'], cc['c']], cc['score'], max(scores[cc['a']]['castle'], scores[cc['b']]['castle'], scores[cc['c']]['castle']) ) for cc in cubic_combinations ] # 3\n",
    "\n",
    "# Remove all LLMs\n",
    "all_combinations = [ c for c in all_combinations if all([ scores[tool]['type'] != 'llm' for tool in c[0] ]) ]\n",
    "\n",
    "\n",
    "all_combinations = sorted(all_combinations, key=lambda x: x[1], reverse=True)\n",
    "all_combinations_toplist = all_combinations[:20]\n",
    "\n",
    "x = [ f'{\" + \".join(c)}' for c, s, m in all_combinations_toplist ]\n",
    "singles = [ m for c, s, m in all_combinations_toplist ]\n",
    "combinations = [ 0 if s-m < 0 else s-m for c, s, m in all_combinations_toplist ]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "single_bar = ax.bar(x, singles, label='Group 1', color='blue')\n",
    "combi_bar = ax.bar(x, combinations, bottom=singles, label='Group 2', color='green')\n",
    "\n",
    "ax.set_xticks(range(len(x)), labels=x, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in combi_bar:\n",
    "    continue\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 20 Combinations')\n",
    "plt.ylabel('CASTLE Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biggest increase in combination scores compared to the single\n",
    "biggest_increase = [ (c, s, m, s-m) for c, s, m in all_combinations_toplist ]\n",
    "biggest_increase = sorted(biggest_increase, key=lambda x: x[3], reverse=True)\n",
    "biggest_increase[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chart of castle scores with combinations\n",
    "top_combination_score_items = sorted(all_combinations, key=lambda x: x[1], reverse=True)[:5]\n",
    "top_combination_score_items = [ (f'{\" + \".join( [display_names[n] for n in top[0]] )}', top[1], tool_color_map['llm']) for top in top_combination_score_items ]\n",
    "all_tool_items = [ (n, scores[n]['castle'], tool_color_map[scores[n]['type']]) for n in scores if scores[n]['type'] != 'llm' ]\n",
    "scores_sorted = sorted(all_tool_items + top_combination_score_items, key=lambda item: item[1], reverse=False)\n",
    "\n",
    "x = [ display_names[ s[0] ] if '+' not in s[0] else s[0] for s in scores_sorted ]\n",
    "y = [ s[1] for s in scores_sorted ]\n",
    "colors = [s[2] for s in scores_sorted]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar = ax.bar(x, y, color=colors)\n",
    "plt.ylabel('CASTLE Score')\n",
    "\n",
    "ax.set_xticks(range(len(x)), labels=x, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=color_map['sast']['color'] , label=color_map['sast']['label']),\n",
    "    Patch(facecolor=color_map['fv']['color'], label=color_map['fv']['label']),\n",
    "    Patch(facecolor=color_map['llm']['color'], label='Combination'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(0.9, 1.15), ncol=3)\n",
    "\n",
    "# set plot size\n",
    "fig = plt.gcf()\n",
    "fig.figure.set_size_inches(8, 4)\n",
    "plt.savefig('assets/tool-combo-castle-scores.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chart of castle scores divided between SCAs and LLMs\n",
    "\n",
    "## 1: SCAs\n",
    "top_combination_score_items = sorted(all_combinations, key=lambda x: x[1], reverse=True)\n",
    "top_combination_score_items = top_combination_score_items[:5]\n",
    "top_combination_score_items = [ (f'{\" + \".join( [display_names[n] for n in top[0]] )}', top[1], tool_color_map['llm']) for top in top_combination_score_items ]\n",
    "all_tool_items = [ (n, scores[n]['castle'], tool_color_map[scores[n]['type']]) for n in scores if scores[n]['type'] != 'llm' ]\n",
    "scores_sorted = sorted(all_tool_items + top_combination_score_items, key=lambda item: item[1], reverse=False)\n",
    "\n",
    "x_sca = [ display_names[ s[0] ] if '+' not in s[0] else s[0] for s in scores_sorted ]\n",
    "y_sca = [ s[1] for s in scores_sorted ]\n",
    "colors_sca = [ tool_color_map[ tool_type_map[s[0]] ] if '+' not in s[0] else tool_color_map['combination'] for s in scores_sorted]\n",
    "\n",
    "## 2: LLMs\n",
    "scores_llm = { k: v for k, v in scores.items() if v['type'] == 'llm' }\n",
    "scores_llm_sorted = sorted(scores_llm.items(), key=lambda item: item[1]['castle'], reverse=False)\n",
    "x_llm = [ display_names[ s[0] ] for s in scores_llm_sorted ]\n",
    "y_llm = [ s[1]['castle'] for s in scores_llm_sorted ]\n",
    "colors_llm = [ tool_color_map['llm'] ] * len(x_llm)\n",
    "\n",
    "## 3: SCAs plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw={'width_ratios': [len(x_sca), len(x_llm)]})\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "\n",
    "bar = ax.bar(x_sca, y_sca, color=colors_sca)\n",
    "plt.ylim(-800, 1000)\n",
    "plt.ylabel('CASTLE Score (C@250)')\n",
    "\n",
    "ax.set_xticks(range(len(x_sca)), labels=x_sca, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "\n",
    "ax.title.set_text('Static Code Analyzers and Tool Combinations')\n",
    "\n",
    "## 4: LLMs plot\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "bar = ax.bar(x_llm, y_llm, color=colors_llm)\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.yaxis.tick_right()\n",
    "\n",
    "ax.set_xticks(range(len(x_llm)), labels=x_llm, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "ax.title.set_text('Large Language Models')\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=tool_color_map['sast'], label='Static Application Security Tester'),\n",
    "    Patch(facecolor=tool_color_map['gca'], label='Generic Code Analyzer'),\n",
    "    Patch(facecolor=tool_color_map['fv'], label='Formal Verification'),\n",
    "    Patch(facecolor=tool_color_map['combination'], label='Tool Combination'),\n",
    "    Patch(facecolor=tool_color_map['llm'], label='Large Language Model'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(1.2, 1.3), ncol=3)\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(hspace=0, wspace=0.05)\n",
    "plt.savefig('assets/castle-scores-separate.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chart of castle scores divided between SCAs and Combinations\n",
    "\n",
    "## 1: SCAs\n",
    "top_combination_score_items = sorted(all_combinations, key=lambda x: x[1], reverse=True)\n",
    "top_combination_score_items = top_combination_score_items[:5]\n",
    "top_combination_score_items = [ (f'{\" + \".join( [display_names[n] for n in top[0]] )}', top[1], color_map['llm']['color']) for top in top_combination_score_items ]\n",
    "all_tool_items = [ (n, scores[n]['castle'], tool_color_map[scores[n]['type']]) for n in scores if scores[n]['type'] != 'llm' ]\n",
    "tool_scores_sorted = sorted(all_tool_items, key=lambda item: item[1], reverse=False)\n",
    "combination_scores_sorted = sorted(top_combination_score_items, key=lambda item: item[1], reverse=False)\n",
    "\n",
    "x_sca = [ display_names[ s[0] ] if '+' not in s[0] else s[0] for s in tool_scores_sorted ]\n",
    "y_sca = [ s[1] for s in tool_scores_sorted ]\n",
    "colors_sca = [ tool_color_map[ tool_type_map[s[0]] ] if '+' not in s[0] else tool_color_map['combination'] for s in tool_scores_sorted]\n",
    "\n",
    "## 2: LLMs\n",
    "x_llm = [ s[0] for s in combination_scores_sorted ]\n",
    "y_llm = [ s[1] for s in combination_scores_sorted ]\n",
    "colors_llm = [ tool_color_map['combination'] ] * len(combination_scores_sorted)\n",
    "\n",
    "## 3: SCAs plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3), gridspec_kw={'width_ratios': [len(x_sca), len(x_llm)]})\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "\n",
    "bar = ax.bar(x_sca, y_sca, color=colors_sca)\n",
    "plt.ylim(0, perfect_castle_score)\n",
    "plt.ylabel('CASTLE Score (C@250)')\n",
    "\n",
    "ax.set_xticks(range(len(x_sca)), labels=x_sca, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    if height < 0:\n",
    "        height = 0\n",
    "        ax.text(rect.get_x() + rect.get_width()/2., height, f'{rect.get_height()}', ha='center', va='bottom', size=8, color='red')\n",
    "        continue\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{rect.get_height()}', ha='center', va='bottom', size=8)\n",
    "    \n",
    "\n",
    "ax.title.set_text('Tools')\n",
    "\n",
    "# Dotted line across the chart at the 200 mark\n",
    "ax.axhline(y=200, color='gray', linestyle='--', linewidth=1)\n",
    "ax.axhline(y=perfect_castle_score, color='green', linestyle='--', linewidth=1)\n",
    "plt.ylim(0, perfect_castle_score + 50)\n",
    "\n",
    "\n",
    "## 4: Combination plot\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "bar = ax.bar(x_llm, y_llm, color=colors_llm)\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.yaxis.tick_right()\n",
    "\n",
    "ax.set_xticks(range(len(x_llm)), labels=x_llm, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "ax.title.set_text('Combinations')\n",
    "ax.axhline(y=200, color='gray', linestyle='--', linewidth=1)\n",
    "ax.axhline(y=perfect_castle_score, color='green', linestyle='--', linewidth=1)\n",
    "plt.ylim(0, perfect_castle_score + 50)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=tool_color_map['sast'], label='Static Application Security Tester'),\n",
    "    Patch(facecolor=tool_color_map['gca'], label='Generic Code Analyzer'),\n",
    "    Patch(facecolor=tool_color_map['fv'], label='Formal Verification'),\n",
    "    Patch(facecolor=tool_color_map['combination'], label='Tool Combination'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(0.4, 1.4), ncol=2)\n",
    "\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(hspace=0, wspace=0.05)\n",
    "plt.savefig('assets/castle-scores-separate.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average SAST recall\n",
    "sast_recall = sum([ scores[tool]['recall'] for tool in scores if scores[tool]['type'] == 'sast' ]) / len([ tool for tool in scores if scores[tool]['type'] == 'sast' ])\n",
    "sast_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.array([ scores[tool]['tp'] for tool in scores if scores[tool]['type'] != 'llm' and scores[tool]['castle'] > 0 ])\n",
    "FP = np.array([ scores[tool]['fp'] for tool in scores if scores[tool]['type'] != 'llm' and scores[tool]['castle'] > 0 ])\n",
    "\n",
    "mask = (TP > 0) & (FP > 0)\n",
    "TP_filtered = TP[mask]\n",
    "FP_filtered = FP[mask]\n",
    "\n",
    "# Log transformation\n",
    "log_TP = np.log(TP_filtered)\n",
    "log_FP = np.log(FP_filtered)\n",
    "\n",
    "# Perform linear regression in log-log space\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(log_TP, log_FP)\n",
    "\n",
    "# Reconstruct the estimated FP values for plotting\n",
    "FP_estimated = np.exp(intercept) * TP_filtered ** slope\n",
    "\n",
    "# Print results\n",
    "print(f\"Log-Log Regression Results:\")\n",
    "print(f\"Exponent (n): {slope:.3f}\")\n",
    "print(f\"Intercept: {intercept:.3f}\")\n",
    "print(f\"R-squared: {r_value**2:.3f}\")\n",
    "\n",
    "# Plot original data vs. fitted line\n",
    "plt.scatter(log_TP, log_FP, label=\"Observed Data\", color=\"blue\")\n",
    "plt.plot(log_TP, slope * log_TP + intercept, color=\"red\", label=f\"Fit: log(FP) = {slope:.2f} log(TP) + {intercept:.2f}\")\n",
    "\n",
    "plt.xlabel(\"log(True Positives)\")\n",
    "plt.ylabel(\"log(False Positives)\")\n",
    "plt.legend()\n",
    "plt.title(\"Log-Log Regression of FP vs. TP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential regression of FP vs. TP\n",
    "TP = np.array([ scores[tool]['tp'] for tool in scores if scores[tool]['type'] != 'llm' and scores[tool]['castle'] > 0 ])\n",
    "FP = np.array([ scores[tool]['fp'] for tool in scores if scores[tool]['type'] != 'llm' and scores[tool]['castle'] > 0 ])\n",
    "\n",
    "mask = (TP > 0) & (FP > 0)\n",
    "TP_filtered = TP[mask]\n",
    "FP_filtered = FP[mask]\n",
    "\n",
    "# Log transformation\n",
    "log_TP = np.log(TP_filtered)\n",
    "log_FP = np.log(FP_filtered)\n",
    "\n",
    "# Perform linear regression in log-log space\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(log_TP, log_FP)\n",
    "\n",
    "# Reconstruct the estimated FP values for plotting\n",
    "FP_estimated = np.exp(intercept) * TP_filtered ** slope\n",
    "\n",
    "# Print results\n",
    "print(f\"Log-Log Regression Results:\")\n",
    "print(f\"Exponent (n): {slope:.3f}\")\n",
    "print(f\"Intercept: {intercept:.3f}\")\n",
    "print(f\"R-squared: {r_value**2:.3f}\")\n",
    "\n",
    "# Plot original data vs. fitted line\n",
    "plt.scatter(log_TP, log_FP, label=\"Observed Data\", color=\"blue\")\n",
    "plt.plot(log_TP, slope * log_TP + intercept, color=\"red\", label=f\"Fit: log(FP) = {slope:.2f} log(TP) + {intercept:.2f}\")\n",
    "\n",
    "plt.xlabel(\"log(True Positives)\")\n",
    "plt.ylabel(\"log(False Positives)\")\n",
    "plt.legend()\n",
    "plt.title(\"Log-Log Regression of FP vs. TP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression of FP vs. TP\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(TP, FP)\n",
    "\n",
    "# Compute fitted values\n",
    "FP_estimated = slope * TP + intercept\n",
    "\n",
    "# Print results\n",
    "print(f\"Linear Regression Results:\")\n",
    "print(f\"Slope (a): {slope:.3f}\")\n",
    "print(f\"Intercept (b): {intercept:.3f}\")\n",
    "print(f\"R-squared: {r_value**2:.3f}\")\n",
    "\n",
    "# Plot data and fitted line\n",
    "plt.scatter(TP, FP, label=\"Observed Data\", color=\"blue\")\n",
    "plt.plot(TP, FP_estimated, color=\"red\", label=f\"Fit: FP = {slope:.2f} TP + {intercept:.2f}\")\n",
    "\n",
    "plt.xlabel(\"True Positives\")\n",
    "plt.ylabel(\"False Positives\")\n",
    "plt.legend()\n",
    "plt.title(\"Linear Regression of FP vs. TP\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic, cubic, exponential and power law models\n",
    "\n",
    "# Quadratic Model\n",
    "quad_coeffs = np.polyfit(TP, FP, 2)  # Fit quadratic: FP = aTP^2 + bTP + c\n",
    "FP_quad = np.polyval(quad_coeffs, TP)\n",
    "r2_quad = r2_score(FP, FP_quad)\n",
    "\n",
    "# Cubic Model\n",
    "cubic_coeffs = np.polyfit(TP, FP, 3)  # Fit cubic: FP = aTP^3 + bTP^2 + cTP + d\n",
    "FP_cubic = np.polyval(cubic_coeffs, TP)\n",
    "r2_cubic = r2_score(FP, FP_cubic)\n",
    "\n",
    "# Exponential Model\n",
    "def exp_model(x, a, b):\n",
    "    return a * np.exp(b * x)\n",
    "\n",
    "exp_params, _ = curve_fit(exp_model, TP, FP, maxfev=5000)\n",
    "FP_exp = exp_model(TP, *exp_params)\n",
    "r2_exp = r2_score(FP, FP_exp)\n",
    "\n",
    "# Power Law Model\n",
    "def power_model(x, a, b):\n",
    "    return a * x ** b\n",
    "\n",
    "power_params, _ = curve_fit(power_model, TP, FP, maxfev=5000)\n",
    "FP_power = power_model(TP, *power_params)\n",
    "r2_power = r2_score(FP, FP_power)\n",
    "\n",
    "# Print R^2 values\n",
    "print(f\"Quadratic R²: {r2_quad:.3f}\")\n",
    "print(f\"Cubic R²: {r2_cubic:.3f}\")\n",
    "print(f\"Exponential R²: {r2_exp:.3f}\")\n",
    "print(f\"Power Law R²: {r2_power:.3f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(TP, FP, label=\"Observed Data\", color=\"blue\")\n",
    "\n",
    "plt.plot(TP, FP_quad, label=f\"Quadratic (R²={r2_quad:.2f})\", linestyle=\"dashed\", color=\"green\")\n",
    "plt.plot(TP, FP_cubic, label=f\"Cubic (R²={r2_cubic:.2f})\", linestyle=\"dashed\", color=\"purple\")\n",
    "plt.plot(TP, FP_exp, label=f\"Exponential (R²={r2_exp:.2f})\", linestyle=\"dashed\", color=\"orange\")\n",
    "plt.plot(TP, FP_power, label=f\"Power Law (R²={r2_power:.2f})\", linestyle=\"dashed\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"True Positives\")\n",
    "plt.ylabel(\"False Positives\")\n",
    "plt.legend()\n",
    "plt.title(\"Testing Different Regression Models\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc dataset statistics\n",
    "\n",
    "def format_number(number):\n",
    "    if int(number) != number:\n",
    "        return f'{number:,.1f}'\n",
    "    return number\n",
    "\n",
    "ds_stats = dataset['statistics']\n",
    "for s in ['line_count', 'functions', 'cyclomatic_complexity', 'halstead_volume', 'maintainability_index', 'cl100k_base_tokens' ]:\n",
    "    name = s.replace('_', ' ').title().capitalize()\n",
    "    min_v = format_number(ds_stats[s]['min'])\n",
    "    max_v = format_number(ds_stats[s]['max'])\n",
    "    avg = format_number(ds_stats[s]['average'])\n",
    "    total = format_number(ds_stats[s]['total'])\n",
    "    print(f'{name} & {min_v} & {avg} & {max_v} & {total} \\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart of castle scores for LLMs\n",
    "llm_scores = { k: v for k, v in scores.items() if v['type'] == 'llm' }\n",
    "llm_scores = sorted(llm_scores.items(), key=lambda item: item[1]['castle'], reverse=False)\n",
    "\n",
    "x = [ display_names[ s[0] ] for s in llm_scores ]\n",
    "y = [ s[1]['castle'] for s in llm_scores ]\n",
    "colors = [tool_color_map[tool_type_map[s[0]]] for s in llm_scores]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "bar = ax.bar(x, y, color=colors)\n",
    "plt.ylabel('CASTLE Score')\n",
    "\n",
    "ax.set_xticks(range(len(x)), labels=x, rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "for rect in bar:\n",
    "    height = rect.get_height()\n",
    "    vertical_anchor = 'top' if height < 0 else 'bottom'\n",
    "    ax.text(rect.get_x() + rect.get_width()/2., height, f'{height}', ha='center', va=vertical_anchor, size=8)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [ Patch(facecolor=color_map[color]['color'], label=color_map[color]['label']) for color in color_map ]\n",
    "ax.legend(handles=legend_elements, bbox_to_anchor=(0.95, 1.1), ncol=3)\n",
    "\n",
    "# Dotted line across the chart at the 200 mark\n",
    "ax.axhline(y=200, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.savefig('assets/castle-scores-llms.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthtetic report with random line numbers\n",
    "import random\n",
    "def gen_synthetic_report():\n",
    "    main_dataset = dataset['tests']\n",
    "    reports = []\n",
    "    for t in main_dataset:\n",
    "        reports.append({\n",
    "            'id': t['id'],\n",
    "            'findings': [\n",
    "                {\n",
    "                    'severity': 'high',\n",
    "                    'line': random.randint(1, t['line_count']),\n",
    "                    'cwe': 0,\n",
    "                    'message': 'Synthetic finding',\n",
    "                }\n",
    "            ],\n",
    "            'report': 'SYNTHETIC'\n",
    "        })\n",
    "        \n",
    "    results = []\n",
    "    for i, rep in enumerate(reports):\n",
    "        findings = filter_findings(rep['findings'])\n",
    "        # Determine result\n",
    "        result = validate_findings(dataset['tests'][i], findings)\n",
    "        results.append(result)\n",
    "        \n",
    "    return castle(top_25_cwes, 1, results)\n",
    "\n",
    "n = 1000\n",
    "synthetic_reports = [ gen_synthetic_report() for _ in range(n) ]\n",
    "sum([ s[5] for s in synthetic_reports ]) / n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
